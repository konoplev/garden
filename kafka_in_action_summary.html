<!DOCTYPE html>
<html>
<head>
    <title>Kafka in action summary</title>

    <!-- meta -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- css -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/ionicons.min.css">
    <link rel="stylesheet" href="css/pace.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/lc_gif_player.css">

    <!-- js -->
    <script src="js/jquery-2.1.3.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/pace.min.js"></script>
    <script src="js/modernizr.custom.js"></script>
</head>

<body id="single">
<div class="container">
    <header id="site-header">
        <div class="row">
            <div class="col-md-4 col-sm-5 col-xs-8">
                <div class="logo">
                    <h1><a href="index.html"><b>Konoplev's</b> garden</a></h1>
                </div>
            </div><!-- col-md-4 -->
            <div class="col-md-8 col-sm-7 col-xs-4">
                <nav class="main-nav" role="navigation">
                    <div class="navbar-header">
                        <button type="button" id="trigger-overlay" class="navbar-toggle">
                            <span class="ion-navicon"></span>
                        </button>
                    </div>

                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav navbar-right">
                            <li class="cl-effect-11"><a href="index.html" data-hover="Garden">Garden</a></li>
                            <li class="cl-effect-11"><a href="http://konoplev.me" data-hover="Blog">Blog</a></li>
                        </ul>
                    </div><!-- /.navbar-collapse -->
                </nav>
                <div id="header-search-box">
                    <a id="search-menu" href="#"><span id="search-icon" class="ion-ios-search-strong"></span></a>
                    <div id="search-form" class="search-form">
                        <form role="search" method="get" id="searchform" action="#">
                            <input type="search" placeholder="Search" required class="search-input">
                            <button type="submit"><span class="ion-ios-search-strong"></span></button>
                        </form>
                    </div>
                </div>
            </div><!-- col-md-8 -->
        </div>
    </header>
</div>


<div class="content-body">
    <div class="container">
        <div class="row">
            <main class="col-md-12">
                <div id="content-holder">
                    <article class="post post-1">
                        <header class="entry-header">
                            <h1 class="entry-title">Kafka in action summary</h1>
                            <div class="entry-meta">
                            <span class="post-date"><a href="#"><time class="entry-date"
                                                                      datetime="2020-12-03T23:00:00Z">Dec 4, 2020</time></a></span>

                                <span class="post-category">
                                                                        <a href="queues.html"> #queues</a>
                                                                        <a href="programming.html"> #programming</a>
                                                                    </span>

                                                            </div>
                        </header>
                        <div class="entry-content clearfix">
                            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Summary of Kafka in action book</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction">Introduction</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_publish_subscribe_messaging">Publish/Subscribe Messaging</h3>
<div class="paragraph">
<p>Publish/subscribe messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.</p>
</div>
</div>
<div class="sect2">
<h3 id="_central_queue">Central Queue</h3>
<div class="paragraph">
<p>Design without queue</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_0.png" alt="kafka 0">
</div>
</div>
<div class="paragraph">
<p>Applications provide metrics to some service.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_1.png" alt="kafka 1">
</div>
</div>
<div class="paragraph">
<p>Many metrics publishers, using direct connections.</p>
</div>
<div class="paragraph">
<p>We introduce A metrics publish/subscribe system.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_2.png" alt="kafka 2">
</div>
</div>
<div class="paragraph">
<p>You would like to have is a single centralized system that allows for publishing generic types of data, which will grow as your business grows.</p>
</div>
</div>
<div class="sect2">
<h3 id="_kafka">Kafka</h3>
<div class="sect3">
<h4 id="_messages_and_batches">Messages and Batches</h4>
<div class="paragraph">
<p>The unit of data within Kafka is called a message. If you are approaching Kafka from a database background, you can think of this as similar to a row or a record. A message is simply an array of bytes as far as Kafka is concerned, so the data contained within it does not have a specific format or meaning to Kafka. A message can have an optional bit of metadata, which is referred to as a key. The key is also a byte array and, as with the message, has no specific meaning to Kafka. Keys are used when messages are to be written to partitions in a more controlled manner. The simplest such scheme is to generate a consistent hash of the key, and then select the partition number for that message by taking the result of the hash modulo, the total number of partitions in the topic. This assures that messages with the same key are always written to the same partition.</p>
</div>
<div class="paragraph">
<p>For efficiency, messages are written into Kafka in batches. A batch is just a collection of messages, all of which are being produced to the same topic and partition.</p>
</div>
</div>
<div class="sect3">
<h4 id="_schemas">Schemas</h4>
<div class="paragraph">
<p>While messages are opaque byte arrays to Kafka itself, it is recommended that additional structure, or schema, be imposed on the message content so that it can be easily understood.</p>
</div>
<div class="paragraph">
<p>A consistent data format is important in Kafka, as it allows writing and reading messages to be decoupled. When these tasks are tightly coupled, applications that subscribe to messages must be updated to handle the new data format, in parallel with the old format. Only then can the applications that publish the messages be updated to utilize the new format.</p>
</div>
</div>
<div class="sect3">
<h4 id="_topics_and_partitions">Topics and Partitions</h4>
<div class="paragraph">
<p>Messages in Kafka are categorized into topics.</p>
</div>
<div class="paragraph">
<p>Topics are additionally broken down into a number of partitions. Going back to the “commit log” description, a partition is a single log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_3.png" alt="kafka 3">
</div>
</div>
<div class="paragraph">
<p>Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.</p>
</div>
<div class="paragraph">
<p>The term stream is often used when discussing data within systems like Kafka. Most often, a stream is considered to be a single topic of data, regardless of the number of partitions. This represents a single stream of data moving from the producers to the consumers.</p>
</div>
</div>
<div class="sect3">
<h4 id="_producers_and_consumers">Producers and Consumers</h4>
<div class="paragraph">
<p>Producers create new messages.</p>
</div>
<div class="paragraph">
<p>By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the producer will direct messages to specific partitions. This is typically done using the message key and a partitioner that will generate a hash of the key and map it to a specific partition.</p>
</div>
<div class="paragraph">
<p>Consumers read messages.</p>
</div>
<div class="paragraph">
<p>The consumer keeps track of which messages it has already consumed by keeping track of the <strong>offset</strong> of messages. The offset is another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place.</p>
</div>
<div class="paragraph">
<p>Consumers work as part of a <strong>consumer group</strong>, which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_4.png" alt="kafka 4">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_exctly_once_semantic_1_11">Exctly once semantic (1.11)</h4>
<div class="paragraph">
<p>Why message can be consumed twice:
- It produced twice (Producer didn&#8217;t receive ACK and sent the message one more time)
- Consumer didn&#8217;t commit offset but read the data. It (or another process) read it one more time.</p>
</div>
<div class="paragraph">
<p>How to solve it:
- Write path
	-- Idempotency
	-- Atomic multi-partition writes
- Read path
	-- Only read commited data
- Exactly once processing
	-- Transactional read-process-write pattern</p>
</div>
<div class="paragraph">
<p>First problem solved on producer&#8217;s side:</p>
</div>
<div class="paragraph">
<p>Enable indepotence producer: enable.indepotence = true (requared ack = all)</p>
</div>
<div class="paragraph">
<p>ProducerId - identifies producer. Sequence - a number allowing to deduplicated</p>
</div>
<div class="paragraph">
<p>To have atomic multi-partition writes Kafka transactions are added:</p>
</div>
<div class="paragraph">
<p>It uses Two-Phase commit. <a href="https://www.youtube.com/watch?v=zm5A7z95pdE" class="bare">https://www.youtube.com/watch?v=zm5A7z95pdE</a></p>
</div>
<div class="paragraph">
<p>It uses Chandy-Lamport Snapshotting allowing to have a consistent snapshot of the system.</p>
</div>
</div>
<div class="sect3">
<h4 id="_brokers_and_clusters">Brokers and Clusters</h4>
<div class="paragraph">
<p>A single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk.</p>
</div>
<div class="paragraph">
<p>Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one broker will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</p>
</div>
<div class="paragraph">
<p>A partition may be assigned to multiple brokers, which will result in the partition being replicated (as seen in Figure 1-7). This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_5.png" alt="kafka 5">
</div>
</div>
<div class="paragraph">
<p>A key feature of Apache Kafka is that of retention, which is the durable storage of messages for some period of time. Kafka brokers are configured with a default retention setting for topics, either retaining messages for some period of time (e.g., 7 days) or until the topic reaches a certain size in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be configured with their own retention settings so that messages are stored for only as long as they are useful. For example, a tracking topic might be retained for several days, whereas application metrics might be retained for only a few hours. Topics can also be configured as log compacted, which means that Kafka will retain only the last message produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting.</p>
</div>
</div>
<div class="sect3">
<h4 id="_multiple_clusters">Multiple Clusters</h4>
<div class="paragraph">
<p>As Kafka deployments grow, it is often advantageous to have multiple clusters. There are several reasons why this can be useful:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Segregation of types of data</p>
</li>
<li>
<p>Isolation for security requirements</p>
</li>
<li>
<p>Multiple datacenters (disaster recovery)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters.</p>
</div>
<div class="paragraph">
<p><strong>MirrorMaker</strong> is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another. Figure shows an example of an architecture that uses MirrorMaker, aggregating messages from two local clusters into an aggregate cluster, and then copying that cluster to other datacenters.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_6.png" alt="kafka 6">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_why_kafka">Why Kafka?</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Multiple Producers<br>
For example, a site that serves content to users via a number of microservices can have a single topic for page views that all services can write to using a common format. Consumer applications can then receive a single stream of page views for all applications on the site without having to coordinate consuming from multiple topics, one for each application.</p>
</li>
<li>
<p>Multiple Consumers<br>
Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.</p>
</li>
<li>
<p>Disk-Based Retention<br>
Messages are committed to disk, and will be stored with configurable retention rules. These options can be selected on a per-topic basis, allowing for different streams of messages to have different amounts of retention depending on the consumer needs. Durable retention means that if a consumer falls behind, either due to slow processing or a burst in traffic, there is no danger of losing data. It also means that maintenance can be performed on consumers, taking applications offline for a short period of time, with no concern about messages backing up on the producer or getting lost. Consumers can be stopped, and the messages will be retained in Kafka. This allows them to restart and pick up processing messages where they left off with no data loss.</p>
</li>
<li>
<p>Scalable<br>
Kafka’s flexible scalability makes it easy to handle any amount of data.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_use_cases">Use cases</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>ACTIVITY TRACKING</p>
</li>
<li>
<p>MESSAGING</p>
</li>
<li>
<p>METRICS AND LOGGING</p>
</li>
<li>
<p>COMMIT LOG<br>
database changes can be published to Kafka and applications can easily monitor this stream to receive live updates as they happen.</p>
</li>
<li>
<p>STREAM PROCESSING<br>
Stream frameworks allow users to write small applications to operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for efficient processing by other applications, or transforming messages using data from multiple sources.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kafka_internals">Kafka internals</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_cluster_membership">Cluster Membership</h3>
<div class="paragraph">
<p>Every time a broker process starts, it registers itself with its ID in Zookeeper by creating an ephemeral node. Different Kafka components subscribe to the /brokers/ids path in Zookeeper where brokers are registered so they get notified when brokers are added or removed.</p>
</div>
<div class="paragraph">
<p>If you try to start another broker with the same ID, you will get an error</p>
</div>
<div class="paragraph">
<p>Even though the node representing the broker is gone when the broker is stopped, the broker ID still exists in other data structures. For example, the list of replicas of each topic (see “Replication”) contains the broker IDs for the replica. This way, if you completely lose a broker and start a brand new broker with the ID of the old one, it will immediately join the cluster in place of the missing broker with the same partitions and topics assigned to it.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_controller">The Controller</h3>
<div class="paragraph">
<p>The controller is one of the Kafka brokers that, in addition to the usual broker functionality, is responsible for electing partition leaders</p>
</div>
<div class="paragraph">
<p>The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called /controller. When other brokers start, they also try to create this node, but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller. The brokers create a Zookeeper watch on the controller node so they get notified of changes to this node. This way, we guarantee that the cluster will only have one controller at a time.</p>
</div>
<div class="paragraph">
<p>Each time a controller is elected, it receives a new, higher controller epoch number through a Zookeeper conditional increment operation. The brokers know the current controller epoch and if they receive a message from a controller with an older number, they know to ignore it.</p>
</div>
<div class="paragraph">
<p>When the controller notices that a broker left the cluster, it knows that all the partitions that had a leader on that broker will need a new leader. It goes over all the partitions that need a new leader, determines who the new leader should be (simply the next replica in the replica list of that partition), and sends a request to all the brokers that contain either the new leaders or the existing followers for those partitions. The request contains information on the new leader and the followers for the partitions. Each new leader knows that it needs to start serving producer and consumer requests from clients while the followers know that they need to start replicating messages from the new leader.</p>
</div>
<div class="paragraph">
<p>When the controller notices that a broker joined the cluster, it uses the broker ID to check if there are replicas that exist on this broker. If there are, the controller notifies both new and existing brokers of the change, and the replicas on the new broker start replicating messages from the existing leaders.</p>
</div>
</div>
<div class="sect2">
<h3 id="_replication">Replication</h3>
<div class="paragraph">
<p>Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</p>
</div>
<div class="paragraph">
<p>There are two types of replicas:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Leader replica<br>
single replica designated as the leader.</p>
</li>
<li>
<p>Follower replica<br>
All replicas for a partition that are not leaders are called followers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In order to stay in sync with the leader, the replicas send the leader <strong>Fetch</strong> requests, the exact same type of requests that consumers send in order to consume messages.</p>
</div>
<div class="paragraph">
<p>Those <strong>Fetch</strong> requests contain the offset of the message that the replica wants to receive next, and will always be in order.</p>
</div>
<div class="paragraph">
<p>the leader can know that a replica got all messages up to message 3 when the replica requests message 4.</p>
</div>
<div class="paragraph">
<p>If a replica hasn’t requested a message in more than 10 seconds or if it has requested messages but hasn’t caught up to the most recent message in more than 10 seconds, the replica is considered <em>out of sync</em>. If a replica fails to keep up with the leader, it can no longer become the new leader in the event of failure—after all, it does not contain all the messages.</p>
</div>
<div class="paragraph">
<p>The amount of time a follower can be inactive or behind before it is considered out of sync is controlled by the <strong>replica.lag.time.max.ms</strong> configuration parameter.</p>
</div>
<div class="paragraph">
<p>A <em>preferred leader</em> — the replica that was the leader when the topic was originally created. It is preferred because when partitions are first created, the leaders are balanced between brokers. As a result, we expect that when the preferred leader is indeed the leader for all partitions in the cluster, load will be evenly balanced between brokers.  By default, Kafka is configured with auto.leader.rebalance.enable=true, which will check if the preferred leader replica is not the current leader but is in-sync and trigger leader election to make the preferred leader the current leader.</p>
</div>
<div class="paragraph">
<p>the output of the kafka-topics.sh tool. The first replica in the list is always the preferred leader.  make sure you spread those around different brokers to avoid overloading some brokers with leaders while other brokers are not handling their fair share of the work.</p>
</div>
</div>
<div class="sect2">
<h3 id="_request_processing">Request Processing</h3>
<div class="paragraph">
<p>All requests sent to the broker from a specific client will be processed in the order in which they were received—this guarantee is what allows Kafka to behave as a message queue and provide ordering guarantees on the messages it stores.</p>
</div>
<div class="paragraph">
<p>All requests have a standard header that includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Request type (also called API key)</p>
</li>
<li>
<p>Request version (so the brokers can handle clients of different versions and respond accordingly)</p>
</li>
<li>
<p>Correlation ID: a number that uniquely identifies the request and also appears in the response and in the error logs (the ID is used for troubleshooting)</p>
</li>
<li>
<p>Client ID: used to identify the application that sent the request</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The most common types of requests are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Produce requests<br>
Sent by producers and contain messages the clients write to Kafka brokers.</p>
</li>
<li>
<p>Fetch requests<br>
Sent by consumers and follower replicas when they read messages from Kafka brokers.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_7.png" alt="kafka 7">
</div>
</div>
<div class="paragraph">
<p>Both produce requests and fetch requests have to be sent to the leader replica of a partition. If a broker receives a produce request for a specific partition and the leader for this partition is on a different broker, the client that sent the produce request will get an error response of “Not a Leader for Partition.” The same error will occur if a fetch request</p>
</div>
<div class="paragraph">
<p>How do the clients know where to send the requests? Kafka clients use another request type called a <em>metadata request</em>, which includes a list of topics the client is interested in. Metadata requests can be sent to any broker because all brokers have a metadata cache that contains this information.</p>
</div>
<div class="paragraph">
<p>They also need to occasionally refresh this information (refresh intervals are controlled by the <strong>metadata.max.age.ms</strong> configuration parameter) by sending another metadata request so they know if the topic metadata changed. If a client receives the “Not a Leader” error to one of its requests, it will refresh its metadata before trying to send the request again.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_8.png" alt="kafka 8">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_produce_requests">Produce Requests</h3>
<div class="paragraph">
<p>a configuration parameter called acks is the number of brokers who need to acknowledge receiving the message before it is considered a successful write. Producers can be configured to consider messages as “written successfully” when the message was accepted by just the leader (acks=1), all in-sync replicas (acks=all), or the moment the message was sent without waiting for the broker to accept it at all (acks=0).</p>
</div>
<div class="paragraph">
<p>When the broker that contains the lead replica for a partition receives a produce request for this partition, it will start by running a few validations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Does the user sending the data have write privileges on the topic?</p>
</li>
<li>
<p>Is the number of acks specified in the request valid (only 0, 1, and “all” are allowed)?</p>
</li>
<li>
<p>If acks is set to all, are there enough in-sync replicas for safely writing the message? (Brokers can be configured to refuse new messages if the number of in-sync replicas falls below a configurable number;</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Then it will write the new messages to local disk. On Linux, the messages are written to the filesystem cache and there is no guarantee about when they will be written to disk. Kafka does not wait for the data to get persisted to disk—it relies on replication for message durability.</p>
</div>
<div class="paragraph">
<p>if acks is set to 0 or 1, the broker will respond immediately; if acks is set to all, the request will be stored in a buffer called <em>purgatory</em> until the leader observes that the follower replicas replicated the message, at which point a response is sent to the client.</p>
</div>
</div>
<div class="sect2">
<h3 id="_fetch_requests">Fetch Requests</h3>
<div class="paragraph">
<p>Brokers process fetch requests in a way that is very similar to the way produce requests are handled.</p>
</div>
<div class="paragraph">
<p>Clients also specify a limit to how much data the broker can return for each partition. The limit is important because clients need to allocate memory that will hold the response sent back from the broker. Without this limit, brokers could send back replies large enough to cause clients to run out of memory.</p>
</div>
<div class="paragraph">
<p>When the leader receives the request, it first checks if the request is valid—does this offset even exist for this particular partition? If the client is asking for a message that is so old that it got deleted from the partition or an offset that does not exist yet, the broker will respond with an error.</p>
</div>
<div class="paragraph">
<p>Kafka famously uses a <strong>zero-copy</strong> method to send the messages to the clients—this means that Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buffers.</p>
</div>
<div class="paragraph">
<p>This technique removes the overhead of copying bytes and managing buffers in memory, and results in much improved performance.</p>
</div>
<div class="paragraph">
<p>In addition to setting an upper boundary on the amount of data the broker can return. “Only return results once you have at least 10K bytes to send me.” This is a great way to reduce CPU and network utilization when clients are reading from topics that are not seeing much traffic</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_9.png" alt="kafka 9">
</div>
</div>
<div class="paragraph">
<p>clients can also define a timeout to tell the broker “If you didn’t satisfy the minimum amount of data to send within x milliseconds, just send what you got.”</p>
</div>
<div class="paragraph">
<p>It is also interesting to note that not all the data that exists on the leader of the partition is available for clients to read. Most clients can only read messages that were written to all in-sync replicas. the leader of the partition knows which messages were replicated to which replica, and until a message was written to all in-sync replicas, it will not be sent to consumers.</p>
</div>
<div class="paragraph">
<p>Messages not replicated to enough replicas yet are considered “unsafe”—if the leader crashes and another replica takes its place, these messages will no longer exist in Kafka. If we allowed clients to read messages that only exist on the leader, we could see inconsistent behavior.</p>
</div>
<div class="paragraph">
<p>This behavior also means that if replication between brokers is slow for some reason, it will take longer for new messages to arrive to consumers (since we wait for the messages to replicate first). This delay is limited to <strong>replica.lag.time.max.ms</strong> — the amount of time a replica can be delayed in replicating new messages while still being considered in-sync.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_10.png" alt="kafka 10">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_other_requests">Other Requests</h3>
<div class="paragraph">
<p>when the controller announces that a partition has a new leader, it sends a <strong>LeaderAndIsr</strong> request to the new leader (so it will know to start accepting client requests) and to the followers (so they will know to follow the new leader).</p>
</div>
<div class="paragraph">
<p>we recommend upgrading the brokers before upgrading any of the clients—new brokers know how to handle old requests, but not vice versa.</p>
</div>
<div class="paragraph">
<p>In release 0.10.0 we added ApiVersionRequest, which allows clients to ask the broker which versions of each request is supported and to use the correct version accordingly. Clients that use this new capability correctly will be able to talk to older brokers by using a version of the protocol that is supported by the broker they are connecting to.</p>
</div>
</div>
<div class="sect2">
<h3 id="_physical_storage">Physical Storage</h3>
<div class="paragraph">
<p>Partitions cannot be split between multiple brokers and not even between multiple disks on the same broker. So the size of a partition is limited by the space available on a single mount point.</p>
</div>
</div>
<div class="sect2">
<h3 id="_partition_allocation">Partition Allocation</h3>
<div class="paragraph">
<p>When you create a topic, Kafka first decides how to allocate the partitions between brokers. Suppose you have 6 brokers and you decide to create a topic with 10 partitions and a replication factor of 3. Kafka now has 30 partition replicas to allocate to 6 brokers. When doing the allocations, the goals are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To spread replicas evenly among brokers—in our example, to make sure we allocate 5 replicas per broker.</p>
</li>
<li>
<p>To make sure that for each partition, each replica is on a different broker. If partition 0 has the leader on broker 2, we can place the followers on brokers 3 and 4, but not on 2 and not both on 3.</p>
</li>
<li>
<p>If the brokers have rack information (available in Kafka release 0.10.0 and higher), then assign the replicas for each partition to different racks if possible. This ensures that an event that causes downtime for an entire rack does not cause complete unavailability for partitions.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To do this, we start with a random broker (let’s say, 4) and start assigning partitions to each broker in round-robin manner to determine the location for the leaders. So partition leader 0 will be on broker 4, partition 1 leader will be on broker 5, partition 2 will be on broker 0 (because we only have 6 brokers), and so on. Then, for each partition, we place the replicas at increasing offsets from the leader. If the leader for partition 0 is on broker 4, the first follower will be on broker 5 and the second on broker 0. The leader for partition 1 is on broker 5, so the first replica is on broker 0 and the second on broker 1.</p>
</div>
<div class="paragraph">
<p>When rack awareness is taken into account, instead of picking brokers in numerical order, we prepare a rack-alternating broker list. Suppose that we know that brokers 0, 1, and 2 are on the same rack, and brokers 3, 4, and 5 are on a separate rack. Instead of picking brokers in the order of 0 to 5, we order them as 0, 3, 1, 4, 2, 5—each broker is followed by a broker from a different rack (Figure 11 below). In this case, if the leader for partition 0 is on broker 4, the first replica will be on broker 2, which is on a completely different rack. This is great, because if the first rack goes offline, we know that we still have a surviving replica and therefore the partition is still available. This will be true for all our replicas, so we have guaranteed availability in the case of rack failure.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_11.png" alt="kafka 11">
</div>
</div>
<div class="paragraph">
<p>which directory to use for the new partitions. the rule is very simple: we count the number of partitions on each directory and add the new partition to the directory with the fewest partitions. This means that if you add a new disk, all the new partitions will be created on that disk. This is because, until things balance out, the new disk will always have the fewest partitions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_mind_the_disk_space">MIND THE DISK SPACE</h3>
<div class="paragraph">
<p>Note that the allocation of partitions to brokers does not take available space or existing load into account, and that allocation of partitions to disks takes the number of partitions into account, but not the size of the partitions. This means that if some brokers have more disk space than others (perhaps because the cluster is a mix of older and newer servers), some partitions are abnormally large, or you have disks of different sizes on the same broker</p>
</div>
</div>
<div class="sect2">
<h3 id="_file_management">File Management</h3>
<div class="paragraph">
<p>Retention is an important concept in Kafka—Kafka does not keep data forever.</p>
</div>
<div class="paragraph">
<p>we instead split each partition into segments. By default, each segment contains either 1 GB of data or a week of data, whichever is smaller. As a Kafka broker is writing to a partition, if the segment limit is reached, we close the file and start a new one.</p>
</div>
<div class="paragraph">
<p>The segment we are currently writing to is called an <em>active segment</em>. The active segment is never deleted, so if you set log retention to only store a day of data but each segment contains five days of data, you will really keep data for five days because we can’t delete the data before the segment is closed.</p>
</div>
<div class="paragraph">
<p>Kafka broker will keep an open file handle to every segment in every partition—even inactive segments. This leads to an usually high number of open file handles, and the OS must be tuned accordingly.</p>
</div>
</div>
<div class="sect2">
<h3 id="_file_format">File Format</h3>
<div class="paragraph">
<p>Each segment is stored in a single data file. Inside the file, we store Kafka messages and their offsets. The format of the data on the disk is identical to the format of the messages that we send from the producer to the broker and later from the broker to the consumers. Using the same message format on disk and over the wire is what allows Kafka to use zero-copy optimization when sending messages to consumers and also avoid decompressing and recompressing messages that the producer already compressed.</p>
</div>
<div class="paragraph">
<p>Each message contains—in addition to its key, value, and offset—things like the message size, checksum code that allows us to detect corruption, magic byte that indicates the version of the message format, compression codec (Snappy, GZip, or LZ4), and a timestamp (added in release 0.10.0).</p>
</div>
<div class="paragraph">
<p>If the producer is sending compressed messages, all the messages in a single producer batch are compressed together and sent as the “value” of a “wrapper message” (Figure 5-6). So the broker receives a single message, which it sends to the consumer. But when the consumer decompresses the message value, it will see all the messages that were contained in the batch, with their own timestamps and offsets.</p>
</div>
<div class="paragraph">
<p>This means that if you are using compression on the producer (recommended!), sending larger batches means better compression both over the network and on the broker disks. This also means that if we decide to change the message format that consumers use (e.g., add a timestamp to the message), both the wire protocol and the on-disk format need to change, and Kafka brokers need to know how to handle cases in which files contain messages of two formats due to upgrades.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_12.png" alt="kafka 12">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">bin/kafka-run-class.sh kafka.tools.DumpLogSegments</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_indexes">Indexes</h3>
<div class="paragraph">
<p>In order to help brokers quickly locate the message for a given offset, Kafka maintains an index for each partition. The index maps offsets to segment files and positions within the file.</p>
</div>
<div class="paragraph">
<p>Indexes are also broken into segments, so we can delete old index entries when the messages are purged. If the index becomes corrupted, it will get regenerated from the matching log segment simply by rereading the messages and recording the offsets and locations. It is also completely safe for an administrator to delete index segments if needed—they will be regenerated automatically.</p>
</div>
</div>
<div class="sect2">
<h3 id="_compaction">Compaction</h3>
<div class="paragraph">
<p>Kafka supports such use cases by allowing the retention policy on a topic to be delete, which deletes events older than retention time, to compact, which only stores the most recent value for each key in the topic. Obviously, setting the policy to compact only makes sense on topics for which applications produce events that contain both a key and a value. If the topic contains null keys, compaction will fail.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_compaction_works">How Compaction Works</h3>
<div class="paragraph">
<p>Each log is viewed as split into two portions .</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_13.png" alt="kafka 13">
</div>
</div>
<div class="paragraph">
<p>If compaction is enabled when Kafka starts (using the awkwardly named log.cleaner.enabled configuration), each broker will start a compaction manager thread and a number of compaction threads. These are responsible for performing the compaction tasks. Each of these threads chooses the partition with the highest ratio of dirty messages to total partition size and cleans this partition.</p>
</div>
<div class="paragraph">
<p>To compact a partition, the cleaner thread reads the dirty section of the partition and creates an in-memory map. Each map entry is comprised of a 16-byte hash of a message key and the 8-byte offset of the previous message that had this same key. This means each map entry only uses 24 bytes. If we look at a 1 GB segment and assume that each message in the segment takes up 1 KB, the segment will contain 1 million such messages and we will only need a 24 MB map to compact the segment (we may need a lot less—if the keys repeat themselves, we will reuse the same hash entries often and use less memory). This is quite efficient!</p>
</div>
<div class="paragraph">
<p>When configuring Kafka, the administrator configures how much memory compaction threads can use for this offset map. Even though each thread has its own map, the configuration is for total memory across all threads. If you configured 1 GB for the compaction offset map and you have five cleaner threads, each thread will get 200 MB for its own offset map. Kafka doesn’t require the entire dirty section of the partition to fit into the size allocated for this map, but at least one full segment has to fit. If it doesn’t, Kafka will log an error and the administrator will need to either allocate more memory for the offset maps or use fewer cleaner threads. If only a few segments fit, Kafka will start by compacting the oldest segments that fit into the map. The rest will remain dirty and wait for the next compaction.</p>
</div>
<div class="paragraph">
<p>Once the cleaner thread builds the offset map, it will start reading off the clean segments, starting with the oldest, and check their contents against the offset map. For each message it checks, if the key of the message exists in the offset map. If the key does not exist in the map, the value of the message we’ve just read is still the latest and we copy over the message to a replacement segment. If the key does exist in the map, we omit the message because there is a message with an identical key but newer value later in the partition. Once we’ve copied over all the messages that still contain the latest value for their key, we swap the replacement segment for the original and move on to the next segment. At the end of the process, we are left with one message per key—the one with the latest value.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_14.png" alt="kafka 14">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_deleted_events">Deleted Events</h3>
<div class="paragraph">
<p>In order to delete a key from the system completely, not even saving the last message, the application must produce a message that contains that key and a null value. When the cleaner thread finds such a message, it will first do a normal compaction and retain only the message with the null value. It will keep this special message (known as a tombstone) around for a configurable amount of time. During this time, consumers will be able to see this message and know that the value is deleted.</p>
</div>
</div>
<div class="sect2">
<h3 id="_when_are_topics_compacted">When Are Topics Compacted?</h3>
<div class="paragraph">
<p>In the same way that the delete policy never deletes the current active segments, the compact policy never compacts the current segment. Messages are eligble for compaction only on inactive segments.</p>
</div>
<div class="paragraph">
<p>In version 0.10.0 and older, Kafka will start compacting when 50% of the topic contains dirty records. The goal is not to compact too often (since compaction can impact the read/write performance on a topic), but also not leave too many dirty records around (since they consume disk space). Wasting 50% of the disk space used by a topic on dirty records and then compacting them in one go seems like a reasonable trade-off, and it can be tuned by the administrator.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_reliable_data_delivery">Reliable Data Delivery</h2>
<div class="sectionbody">
<div class="paragraph">
<p>what does Apache Kafka guarantee?
- Kafka provides order guarantee of messages in a partition. If message B was written after message A, using the same producer in the same partition, then Kafka guarantees that the offset of message B will be higher than message A, and that consumers will read message B after message A.
- Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas (but not necessarily flushed to disk). Producers can choose to receive acknowledgments of sent messages when the message was fully committed, when it was written to the leader, or when it was sent over the network.
- Messages that are committed will not be lost as long as at least one replica remains alive.
- Consumers can only read messages that are committed.</p>
</div>
<div class="sect2">
<h3 id="_replication_2">Replication</h3>
<div class="paragraph">
<p>A replica is considered in-sync if it is the leader for a partition, or if it is a follower that:
- Has an active session with Zookeeper—meaning, it sent a heartbeat to Zookeeper in the last 6 seconds (configurable).
- Fetched messages from the leader in the last 10 seconds (configurable).
- Fetched the most recent messages from the leader in the last 10 seconds. That is, it isn’t enough that the follower is still getting messages from the leader; it must have almost no lag.</p>
</div>
<div class="paragraph">
<p>An in-sync replica that is slightly behind can slow down producers and consumers—since they wait for all the in-sync replicas to get the message before it is <em>committed</em>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_broker_configuration">Broker Configuration</h3>
<div class="sect3">
<h4 id="_replication_factor">Replication Factor</h4>
<div class="paragraph">
<p>If you are totally OK with a specific topic being unavailable when a single broker is restarted (which is part of the normal operations of a cluster), then a replication factor of 1 may be enough.</p>
</div>
<div class="paragraph">
<p>For those reasons, we recommend a replication factor of 3 for any topic where availability is an issue.</p>
</div>
<div class="paragraph">
<p>To protect against rack-level misfortune, we recommend placing brokers in multiple racks and using the broker.rack broker configuration parameter to configure the rack name for each broker. If rack names are configured, Kafka will make sure replicas for a partition are spread across multiple racks in order to guarantee even higher availability.</p>
</div>
</div>
<div class="sect3">
<h4 id="_unclean_leader_election">Unclean Leader Election</h4>
<div class="paragraph">
<p><strong>unclean.leader.election.enable</strong> (by default it is set to true).</p>
</div>
<div class="paragraph">
<p>This leader election is “clean” in the sense that it guarantees no loss of committed data—by definition, committed data exists on all in-sync replicas.</p>
</div>
<div class="paragraph">
<p>But what do we do when no in-sync replica exists except for the leader that just became unavailable?</p>
</div>
<div class="paragraph">
<p>This situation can happen in one of two scenarios:
- The partition had three replicas, and the two followers became unavailable (let’s say two brokers crashed). In this situation, as producers continue writing to the leader, all the messages are acknowledged and committed (since the leader is the one and only in-sync replica). Now let’s say that the leader becomes unavailable (oops, another broker crash). In this scenario, if one of the out-of-sync followers starts first, we have an out-of-sync replica as the only available replica for the partition.
- The partition had three replicas and, due to network issues, the two followers fell behind so that even though they are up and replicating, they are no longer in sync. The leader keeps accepting messages as the only in-sync replica. Now if the leader becomes unavailable, the two available replicas are no longer in-sync.</p>
</div>
<div class="paragraph">
<p>In both these scenarios, we need to make a difficult decision:
- If we don’t allow the out-of-sync replica to become the new leader, the partition will remain offline until we bring the old leader (and the last in-sync replica) back online. In some cases (e.g., memory chip needs replacement), this can take many hours.
- If we do allow the out-of-sync replica to become the new leader, we are going to lose all messages that were written to the old leader while that replica was out of sync and also cause some inconsistencies in consumers. Why? Imagine that while replicas 0 and 1 were not available, we wrote messages with offsets 100-200 to replica 2 (then the leader). Now replica 3 is unavailable and replica 0 is back online. Replica 0 only has messages 0-100 but not 100-200. If we allow replica 0 to become the new leader, it will allow producers to write new messages and allow consumers to read them. So, now the new leader has completely new messages 100-200. First, let’s note that some consumers may have read the old messages 100-200, some consumers got the new 100-200, and some got a mix of both. This can lead to pretty bad consequences when looking at things like downstream reports. In addition, replica 2 will come back online and become a follower of the new leader. At that point, it will delete any messages it got that are ahead of the current leader. Those messages will not be available to any consumer in the future.</p>
</div>
<div class="paragraph">
<p>In summary, if we allow out-of-sync replicas to become leaders, we risk data loss and data inconsistencies.</p>
</div>
<div class="paragraph">
<p>Setting <strong>unclean.leader.election.enable</strong> to true means we allow out-of-sync replicas to become leaders.</p>
</div>
</div>
<div class="sect3">
<h4 id="_minimum_in_sync_replicas">Minimum In-Sync Replicas</h4>
<div class="paragraph">
<p><strong>min.insync.replicas</strong></p>
</div>
<div class="paragraph">
<p>If this replica becomes unavailable, we may have to choose between availability and consistency. Note that part of the problem is that, per Kafka reliability guarantees, data is considered committed when it is written to all in-sync replicas, even when <strong>all</strong> means just one replica and the data could be lost if that replica is unavailable.</p>
</div>
<div class="paragraph">
<p>If a topic has three replicas and you set <strong>min.insync.replicas</strong> to 2, then you can only write to a partition in the topic if at least two out of the three replicas are in-sync.</p>
</div>
<div class="paragraph">
<p>When all three replicas are in-sync, everything proceeds normally. This is also true if one of the replicas becomes unavailable. However, if two out of three replicas are not available, the brokers will no longer accept produce requests. Instead, producers that attempt to send data will receive NotEnoughReplicasException.</p>
</div>
<div class="paragraph">
<p>In order to recover from this read-only situation, we must make one of the two unavailable partitions available again (maybe restart the broker) and wait for it to catch up and get in-sync.</p>
</div>
</div>
<div class="sect3">
<h4 id="_using_producers_in_a_reliable_system">Using Producers in a Reliable System</h4>
<div class="paragraph">
<p>Here are two example scenarios to demonstrate this:
- We configured the brokers with three replicas, and unclean leader election is disabled. So we should never lose a single message that was committed to the Kafka cluster. However, we configured the producer to send messages with acks=1. We send a message from the producer and it was written to the leader, but not yet to the in-sync replicas. The leader sent back a response to the producer saying “Message was written successfully” and immediately crashes before the data was replicated to the other replicas. The other replicas are still considered in-sync (remember that it takes a while before we declare a replica out of sync) and one of them will become the leader. Since the message was not written to the replicas, it will be lost. But the producing application thinks it was written successfully. The system is consistent because no consumer saw the message (it was never committed because the replicas never got it), but from the producer perspective, a message was lost.
- We configured the brokers with three replicas, and unclean leader election is disabled. We learned from our mistakes and started producing messages with acks=all. Suppose that we are attempting to write a message to Kafka, but the leader for the partition we are writing to just crashed and a new one is still getting elected. Kafka will respond with “Leader not Available.” At this point, if the producer doesn’t handle the error correctly and doesn’t retry until the write is successful, the message may be lost. Once again, this is not a broker reliability issue because the broker never got the message; and it is not a consistency issue because the consumers never got the message either. But if producers don’t handle errors correctly, they may cause message loss.</p>
</div>
<div class="paragraph">
<p>pay attention to:
- Use the correct acks configuration to match reliability requirements
- Handle errors correctly both in configuration and in code</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_producer_retries">Configuring Producer Retries</h4>
<div class="paragraph">
<p>There are two parts to handling errors in the producer: the errors that the producers handle automatically for you and the errors that you as the developer using the producer library must handle.</p>
</div>
<div class="paragraph">
<p>if the broker returns the error code LEADER_NOT_AVAILABLE, the producer can try sending the error again—maybe a new broker was elected and the second attempt will succeed. This means that LEADER_NOT_AVAILABLE is a retriable error. On the other hand, if a broker returns an INVALID_CONFIG exception, trying the same message again will not change the configuration. This is an example of a nonretriable error.</p>
</div>
<div class="paragraph">
<p>Note that retrying to send a failed message often includes a small risk that both messages were successfully written to the broker, leading to duplicates. For example, if network issues prevented the broker acknowledgment from reaching the producer, but the message was successfully written and replicated, the producer will treat the lack of acknowledgment as a temporary network issue and will retry sending the message (since it can’t know that it was received). In that case, the broker will end up having the same message twice. Retries and careful error handling can guarantee that each message will be stored at least once, but in the current version of Apache Kafka (0.10.0), we can’t guarantee it will be stored exactly once. Many real-world applications add a unique identifier to each message to allow detecting duplicates and cleaning them when consuming the messages. Other applications make the messages idempotent—meaning that even if the same message is sent twice, it has no negative impact on correctness. For example, the message “Account value is 110$” is idempotent, since sending it several times doesn’t change the result. The message “Add $10 to the account” is not idempotent, since it changes the result every time you send it.</p>
</div>
</div>
<div class="sect3">
<h4 id="_additional_error_handling">Additional Error Handling</h4>
<div class="paragraph">
<p>as a developer, you must still be able to handle other types of errors. These include:
- Nonretriable broker errors such as errors regarding message size, authorization errors, etc.
- Errors that occur before the message was sent to the broker—for example, serialization errors
- Errors that occur when the producer exhausted all retry attempts or when the available memory used by the producer is filled to the limit due to using all of it to store messages while retrying</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_using_consumers_in_a_reliable_system">Using Consumers in a Reliable System</h3>
<div class="paragraph">
<p>For each partition it is consuming, the consumer stores its current location, so they or another consumer will know where to continue after a restart. The main way consumers can lose messages is when committing offsets for events they’ve read but didn’t completely process yet. This way, when another consumer picks up the work, it will skip those events and they will never get processed. This is why paying careful attention to when and how offsets get committed is critical.</p>
</div>
<div class="sect3">
<h4 id="_important_consumer_configuration_properties_for_reliable_processing">Important Consumer Configuration Properties for Reliable Processing</h4>
<div class="paragraph">
<p><strong>group.id</strong> if two consumers have the same group ID and subscribe to the same topic, each will be assigned a subset of the partitions in the topic and will therefore only read a subset of the messages individually (but all the messages will be read by the group as a whole).</p>
</div>
<div class="paragraph">
<p>The second relevant configuration is auto.offset.reset. This parameter controls what the consumer will do when no offsets were committed (e.g., when the consumer first starts) or when the consumer asks for offsets that don’t exist in the broker (Chapter 4 explains how this can happen). There are only two options here. If you choose earliest, the consumer will start from the beginning of the partition whenever it doesn’t have a valid offset. This can lead to the consumer processing a lot of messages twice, but it guarantees to minimize data loss. If you choose latest, the consumer will start at the end of the partition. This minimizes duplicate processing by the consumer but almost certainly leads to some messages getting missed by the consumer.</p>
</div>
<div class="paragraph">
<p><strong>enable.auto.commit.</strong>
If you do anything fancy like pass records to another thread to process in the background, the automatic commit may commit offsets for records the consumer has read but perhaps did not process yet.</p>
</div>
<div class="paragraph">
<p><strong>auto.commit.interval.ms</strong>
The default is every five seconds.</p>
</div>
</div>
<div class="sect3">
<h4 id="_explicitly_committing_offsets_in_consumers">Explicitly Committing Offsets in Consumers</h4>
<div class="ulist">
<ul>
<li>
<p>ALWAYS COMMIT OFFSETS AFTER EVENTS WERE PROCESSED</p>
</li>
<li>
<p>COMMIT FREQUENCY IS A TRADE-OFF BETWEEN PERFORMANCE AND NUMBER OF DUPLICATES IN THE EVENT OF A CRASH</p>
</li>
<li>
<p>MAKE SURE YOU KNOW EXACTLY WHAT OFFSETS YOU ARE COMMITTING</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_rebalances">REBALANCES</h4>
<div class="paragraph">
<p>consumer rebalances will happen and you need to handle them properly. committing offsets before partitions are revoked and cleaning any state you maintain when you are assigned new partitions.</p>
</div>
</div>
<div class="sect3">
<h4 id="_consumers_may_need_to_retry">CONSUMERS MAY NEED TO RETRY</h4>
<div class="paragraph">
<p>Note that unlike traditional pub/sub messaging systems, you commit offsets and not ack individual messages. This means that if you failed to process record \#30 and succeeded in processing record \#31, you should not commit record \#31—this would result in committing all the records up to \#31 including \#30, which is usually not what you want. Instead, try following one of the following two patterns.</p>
</div>
<div class="paragraph">
<p>One option, when you encounter a retriable error, is to commit the last record you processed successfully. Then store the records that still need to be processed in a buffer (so the next poll won’t override them) and keep trying to process the records. You may need to keep polling while trying to process all the records (refer to Chapter 4 for an explanation). You can use the consumer pause() method to ensure that additional polls won’t return additional data to make retrying easier.</p>
</div>
<div class="paragraph">
<p>A second option is, when encountering a retriable error, to write it to a separate topic and continue. A separate consumer group can be used to handle retries from the retry topic, or one consumer can subscribe to both the main topic and to the retry topic, but pause the retry topic between retries. This pattern is similar to the dead-letter-queue system used in many messaging systems.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_validating_system_reliability">Validating System Reliability</h3>
<div class="sect3">
<h4 id="_validating_configuration">Validating Configuration</h4>
<div class="paragraph">
<p>Kafka includes two important tools to help with this validation. The org.apache.kafka.tools package includes VerifiableProducer and VerifiableConsumer classes. These can run as command-line tools, or be embedded in an automated testing framework.</p>
</div>
<div class="paragraph">
<p>The idea is that the verifiable producer produces a sequence of messages containing numbers from 1 to a value you choose. You can configure it the same way you configure your own producer, setting the right number of acks, retries, and rate at which the messages will be produced. When you run it, it will print success or error for each message sent to the broker, based on the acks received. The verifiable consumer performs the complementary check. It consumes events (usually those produced by the verifiable producer) and prints out the events it consumed in order. It also prints information regarding commits and rebalances.</p>
</div>
<div class="paragraph">
<p>You should also consider which tests you want to run. For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Leader election: what happens if I kill the leader? How long does it take the producer and consumer to start working as usual again?</p>
</li>
<li>
<p>Controller election: how long does it take the system to resume after a restart of the controller?</p>
</li>
<li>
<p>Rolling restart: can I restart the brokers one by one without losing any messages?</p>
</li>
<li>
<p>Unclean leader election test: what happens when we kill all the replicas for a partition one by one (to make sure each goes out of sync) and then start a broker that was out of sync? What needs to happen in order to resume operations? Is this acceptable?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Apache Kafka source repository includes an extensive test suite. Many of the tests in the suite are based on the same principle—use the verifiable producer and consumer to make sure rolling upgrades work, for example.</p>
</div>
</div>
<div class="sect3">
<h4 id="_validating_applications">Validating Applications</h4>
<div class="paragraph">
<p>we recommend running tests under a variety of failure conditions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Clients lose connectivity to the server (your system administrator can assist you in simulating network failures)</p>
</li>
<li>
<p>Leader election</p>
</li>
<li>
<p>Rolling restart of brokers</p>
</li>
<li>
<p>Rolling restart of consumers</p>
</li>
<li>
<p>Rolling restart of producers</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_monitoring_reliability_in_production">Monitoring Reliability in Production</h4>
<div class="paragraph">
<p>For the producers, the two metrics most important for reliability are error-rate and retry-rate per record (aggregated). you may want to increase the number of retries—or solve the problem that caused the errors in the first place.</p>
</div>
<div class="paragraph">
<p>On the consumer side, the most important metric is consumer lag. This metric indicates how far the consumer is from the latest message committed to the partition on the broker. Ideally, the lag would always be zero and the consumer will always read the latest message.</p>
</div>
<div class="paragraph">
<p>starting with version 0.10.0, all messages include a timestamp that indicates when the event was produced.</p>
</div>
<div class="paragraph">
<p>The consumers need to both record the number of events consumed (also events per second) and also record lags from the time events were produced to the time they were consumed, using the event timestamp. Then you will need a system to reconcile the events per second numbers from both the producer and the consumer (to make sure no messages were lost on the way) and to make sure the time gaps between the time events were produced in a reasonable amount of time.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_building_data_pipelines">Building Data Pipelines</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_transformations">Transformations</h3>
<div class="paragraph">
<p>ETL, which stands for Extract-Transform-Load, means the data pipeline is responsible for making modifications to the data as it passes through.</p>
</div>
<div class="paragraph">
<p>The main drawback of this approach is that the transformations that happen to the data in the pipeline tie the hands of those who wish to process the data farther down the pipe. If the person who built the pipeline between MongoDB and MySQL decided to filter certain events or remove fields from records, all the users and applications who access the data in MySQL will only have access to partial data. If they require access to the missing fields, the pipeline needs to be rebuilt and historical data will require reprocessing (assuming it is available).</p>
</div>
<div class="paragraph">
<p>ELT stands for Extract-Load-Transform and means the data pipeline does only minimal transformation (mostly around data type conversion), with the goal of making sure the data that arrives at the target is as similar as possible to the source data. These are also called high-fidelity pipelines or data-lake architecture. In these systems, the target system collects “raw data” and all required processing is done at the target system.</p>
</div>
<div class="paragraph">
<p>The drawback is that the transformations take CPU and storage resources at the target system. In some cases, these systems are expensive and there is strong motivation to move computation off those systems when possible.</p>
</div>
</div>
<div class="sect2">
<h3 id="_coupling_and_agility">Coupling and Agility</h3>
<div class="paragraph">
<p>One of the most important goals of data pipelines is to decouple the data sources and data targets.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ad-hoc pipelines<br>
Some companies end up building a custom pipeline for each pair of applications they want to connect. For example, they use Logstash to dump logs to Elasticsearch, Flume to dump logs to HDFS, GoldenGate to get data from Oracle to HDFS, Informatica to get data from MySQL and XMLs to Oracle, and so on. every new system the company adopts will require building additional pipelines, increasing the cost of adopting new technology, and inhibiting innovation.</p>
</li>
<li>
<p>Loss of metadata</p>
</li>
<li>
<p>Extreme processing</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_when_to_use_kafka_connect_versus_producer_and_consumer">When to Use Kafka Connect Versus Producer and Consumer</h3>
<div class="paragraph">
<p>You will use Connect to connect Kafka to datastores that you did not write and whose code you cannot or will not modify. Connect will be used to pull data from the external datastore into Kafka or push data from Kafka to an external store. For datastores where a connector already exists, Connect can be used by nondevelopers, who will only need to configure the connectors.</p>
</div>
<div class="paragraph">
<p>Connect is recommended because it provides out-of-the-box features like configuration management, offset storage, parallelization, error handling, support for different data types, and standard management REST APIs.</p>
</div>
</div>
<div class="sect2">
<h3 id="_kafka_connect">Kafka Connect</h3>
<div class="paragraph">
<p>Kafka Connect is a part of Apache Kafka and provides a scalable and reliable way to move data between Kafka and other datastores. It provides APIs and a runtime to develop and run connector plugins—libraries that Kafka Connect executes and which are responsible for moving the data. Kafka Connect runs as a cluster of worker processes. You install the connector plugins on the workers and then use a REST API to configure and manage connectors, which run with a specific configuration. Connectors start additional tasks to move large amounts of data in parallel and use the available resources on the worker nodes more efficiently. Source connector tasks just need to read data from the source system and provide Connect data objects to the worker processes. Sink connector tasks get connector data objects from the workers and are responsible for writing them to the target data system. Kafka Connect uses convertors to support storing those data objects in Kafka in different formats—JSON format support is part of Apache Kafka, and the Confluent Schema Registry provides Avro converters. This allows users to choose the format in which data is stored in Kafka independent of the connectors they use.</p>
</div>
</div>
<div class="sect2">
<h3 id="_running_connect">Running Connect</h3>
<div class="paragraph">
<p>Starting a Connect worker is very similar to starting a broker—you call the start script with a properties file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"> bin/connect-distributed.sh config/connect-distributed.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are a few key configurations for Connect workers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>bootstrap.servers</em>:: A list of Kafka brokers that Connect will work with. Connectors will pipe their data either to or from those brokers. You don’t need to specify every broker in the cluster, but it’s recommended to specify at least three.</p>
</li>
<li>
<p><em>group.id</em>:: All workers with the same group ID are part of the same Connect cluster. A connector started on the cluster will run on any worker and so will its tasks.</p>
</li>
<li>
<p><em>key.converter</em> and <em>value.converter</em>:: Connect can handle multiple data formats stored in Kafka. The two configurations set the converter for the key and value part of the message that will be stored in Kafka. The default is JSON format using the JSONConverter included in Apache Kafka. These configurations can also be set to AvroConverter, which is part of the Confluent Schema Registry.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Once the workers are up and you have a cluster, make sure it is up and running by checking the REST API:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">gwen$ curl http://localhost:8083/
{"version":"0.10.1.0-SNAPSHOT","commit":"561f45d747cd2a8c"}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">gwen$ curl http://localhost:8083/connector-plugins

[{"class":"org.apache.kafka.connect.file.FileStreamSourceConnector"},{"class":"org.apache.kafka.connect.file.FileStreamSinkConnector"}]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Take note that Kafka Connect also has a standalone mode. It is similar to distributed mode—you just run bin/connect-standalone.sh instead of bin/connect-distributed.sh.</p>
</div>
</div>
<div class="sect2">
<h3 id="_connectors_and_tasks">CONNECTORS AND TASKS</h3>
<div class="paragraph">
<p>Connector plugins implement the connector API, which includes two parts:</p>
</div>
<div class="sect3">
<h4 id="_connectors">Connectors</h4>
<div class="paragraph">
<p>The connector is responsible for three important things:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Determining how many tasks will run for the connector</p>
</li>
<li>
<p>Deciding how to split the data-copying work between the tasks</p>
</li>
<li>
<p>Getting configurations for the tasks from the workers and passing it along</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_tasks">Tasks</h4>
<div class="paragraph">
<p>Tasks are responsible for actually getting the data in and out of Kafka. All tasks are initialized by receiving a context from the worker. Source context includes an object that allows the source task to store the offsets of source records (e.g., in the file connector, the offsets are positions in the file; in the JDBC source connector, the offsets can be primary key IDs in a table). Context for the sink connector includes methods that allow the connector to control the records it receives from Kafka—this is used for things like applying back-pressure, and retrying and storing offsets externally for exactly-once delivery. After tasks are initialized, the are started with a <em>Properties</em> object that contains the configuration the <em>Connector</em> created for the task. Once tasks are started, source tasks poll an external system and return lists of records that the worker sends to Kafka brokers. Sink tasks receive records from Kafka through the worker and are responsible for writing the records to an external system.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_workers">WORKERS</h3>
<div class="paragraph">
<p>Kafka Connect’s worker processes are the “container” processes that execute the connectors and tasks. They are responsible for handling the HTTP requests that define connectors and their configuration, as well as for storing the connector configuration, starting the connectors and their tasks, and passing the appropriate configurations along. If a worker process is stopped or crashes, other workers in a Connect cluster will recognize that (using the heartbeats in Kafka’s consumer protocol) and reassign the connectors and tasks that ran on that worker to the remaining workers. If a new worker joins a Connect cluster, other workers will notice that and assign connectors or tasks to it to make sure load is balanced among all workers fairly. Workers are also responsible for automatically committing offsets for both source and sink connectors and for handling retries when tasks throw errors.</p>
</div>
</div>
<div class="sect2">
<h3 id="_converters_and_connect_s_data_model">CONVERTERS AND CONNECT’S DATA MODEL</h3>
<div class="paragraph">
<p>The last piece of the Connect API puzzle is the connector data model and the converters. Kafka’s Connect APIs includes a data API, which includes both data objects and a schema that describes that data. For example, the JDBC source reads a column from a database and constructs a Connect Schema object based on the data types of the columns returned by the database. It then uses the schema to construct a Struct that contains all the fields in the database record. For each column, we store the column name and the value in that column. Every source connector does something similar—read an event from the source system and generate a pair of Schema and Value. Sink connectors do the opposite—get a Schema and Value pair and use the Schema to parse the values and insert them into the target system.</p>
</div>
<div class="paragraph">
<p>When users configure the worker (or the connector), they choose which converter they want to use to store data in Kafka. At the moment the available choices are Avro, JSON, or strings.</p>
</div>
</div>
<div class="sect2">
<h3 id="_offset_management">OFFSET MANAGEMENT</h3>
<div class="paragraph">
<p>For source connectors, this means that the records the connector returns to the Connect workers include a logical partition and a logical offset. Those are not Kafka partitions and Kafka offsets, but rather partitions and offsets as needed in the source system.</p>
</div>
<div class="paragraph">
<p>In a JDBC source, a partition can be a database table and the offset can be an ID of a record in the table.</p>
</div>
<div class="paragraph">
<p>When the source connector returns a list of records, which includes the source partition and offset for each record, the worker sends the records to Kafka brokers. If the brokers successfully acknowledge the records, the worker then stores the offsets of the records it sent to Kafka. The storage mechanism is pluggable and is usually a Kafka topic. This allows connectors to start processing events from the most recently stored offset after a restart or a crash.</p>
</div>
<div class="paragraph">
<p>Sink connectors have an opposite but similar workflow: they read Kafka records, which already have a topic, partition, and offset identifiers. Then they call the <em>connector.put()</em> method that should store those records in the destination system. If the connector reports success, they commit the offsets they’ve given to the connector back to Kafka, using the usual consumer commit methods.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cross_cluster_data_mirroring">Cross-Cluster Data Mirroring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka’s built-in cross-cluster replicator is called MirrorMaker.</p>
</div>
<div class="paragraph">
<p>Use Cases of Cross-Cluster Mirroring
- Regional and central clusters
- Redundancy (DR)
- Cloud migrations</p>
</div>
<div class="sect2">
<h3 id="_hub_and_spokes_architecture">Hub-and-Spokes Architecture</h3>
<div class="paragraph">
<p>The hub-and-spokes architecture:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_15.png" alt="kafka 15">
</div>
</div>
<div class="paragraph">
<p>A simpler version of the hub-and-spokes architecture:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_16.png" alt="kafka 16">
</div>
</div>
<div class="paragraph">
<p>The main benefit of this architecture is that data is always produced to the local data-center and that events from each datacenter are only mirrored once—to the central datacenter. Applications that process data from a single datacenter can be located at that datacenter. Applications that need to process data from multiple datacenters will be located at the central datacenter where all the events are mirrored. Because replication always goes in one direction and because each consumer always reads from the same cluster, this architecture is simple to deploy, configure, and monitor.</p>
</div>
</div>
<div class="sect2">
<h3 id="_active_active_architecture">Active-Active Architecture</h3>
<div class="imageblock">
<div class="content">
<img src="kafka_17.png" alt="kafka 17">
</div>
</div>
<div class="paragraph">
<p>The main benefits of this architecture are the ability to serve users from a nearby datacenter, which typically has performance benefits, without sacrificing functionality due to limited availability of data (as we’ve seen happen in the hub-and-spokes architecture).</p>
</div>
<div class="paragraph">
<p>The main drawback of this architecture is the challenges in avoiding conflicts when data is read and updated asynchronously in multiple locations. Maintaining data consistency between the two datacenters will be difficult. Here are few examples of the difficulties you will encounter:
- If a user sends an event to one datacenter and reads events from another datacenter, it is possible that the event they wrote hasn’t arrived the second datacenter yet. The developers usually find a way to “stick” each user to a specific datacenter and make sure they use the same cluster most of the time
- An event from one datacenter says user ordered book A and an event from more or less the same time at a second datacenter says that the same user ordered book B. After mirroring, both datacenters have both events and thus we can say that each datacenter has two conflicting events. Applications on both datacenters need to know how to deal with this situation.</p>
</div>
<div class="paragraph">
<p>If you find ways to handle the challenges of asynchronous reads and writes to the same data set from multiple locations, then this architecture is highly recommended.</p>
</div>
<div class="paragraph">
<p>Part of the challenge of active-active mirroring, especially with more than two datacenters, is that you will need a mirroring process for each pair of datacenters and each direction. With five datacenters, you need to maintain at least 20 mirroring processes—and more likely 40, since each process needs redundancy for high availability.</p>
</div>
<div class="paragraph">
<p>In addition, you will want to avoid loops in which the same event is mirrored back-and-forth endlessly. You can do this by giving each “logical topic” a separate topic for each datacenter and making sure to avoid replicating topics that originated in remote datacenters. For example, logical topic users will be topic SF.users in one datacenter and NYC.users in another datacenter. The mirroring processes will mirror topic SF.users from SF to NYC and topic NYC.users from NYC to SF. As a result, each event will only be mirrored once, but each datacenter will contain both SF.users and NYC.users, which means each datacenter will have information for all the users. Consumers will need to consume events from .users if they wish to consume all user events. Another way to think of this setup is to see it as a separate namespace for each datacenter that contains all the topics for the specific datacenter. In our example, we’ll have the NYC and the SF namespaces.</p>
</div>
<div class="paragraph">
<p>Note that in the near future (and perhaps before you read this book), Apache Kafka will add record headers. This will allow tagging events with their originating datacenter and using this header information to avoid endless mirroring loops and also to allow processing events from different datacenters separately.</p>
</div>
</div>
<div class="sect2">
<h3 id="_active_standby_architecture">Active-Standby Architecture</h3>
<div class="paragraph">
<p>In some cases, the only requirement for multiple clusters is to support some kind of disaster scenario.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_18.png" alt="kafka 18">
</div>
</div>
<div class="paragraph">
<p>The disadvantages are waste of a good cluster and the fact that failover between Kafka clusters is, in fact, much harder than it looks.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_loss_and_inconsistencies_in_unplanned_failover">DATA LOSS AND INCONSISTENCIES IN UNPLANNED FAILOVER</h3>
<div class="paragraph">
<p>Because Kafka’s various mirroring solutions are all asynchronous, the DR cluster will not have the latest messages from the primary cluster. You should always monitor how far behind the DR cluster is and never let it fall too far behind. But in a busy system you should expect the DR cluster to be a few hundred or even a few thousand messages behind the primary. If your Kafka cluster handles 1 million messages a second and there is a 5 millisecond lag between the primary and the DR cluster is 5 milliseconds, your DR cluster will be 5,000 messages behind the primary in the best-case scenario.</p>
</div>
</div>
<div class="sect2">
<h3 id="_start_offset_for_applications_after_failover">START OFFSET FOR APPLICATIONS AFTER FAILOVER</h3>
<div class="paragraph">
<p>Perhaps the most challenging part in failing over to another cluster is making sure applications know where to start consuming data. There are several common approaches. Some are simple but can cause additional data loss or duplicate processing; others are more involved but minimize additional data loss and reprocessing.</p>
</div>
<div class="sect3">
<h4 id="_auto_offset_reset">Auto offset reset</h4>
<div class="paragraph">
<p>If your application handles duplicates with no issues, or missing some data is no big deal, this option is by far the easiest. Simply skipping to the end of the topic on failover is probably still the most popular failover method.</p>
</div>
</div>
<div class="sect3">
<h4 id="_replicate_offsets_topic">Replicate offsets topic</h4>
<div class="paragraph">
<p>If you are using new (0.9 and above) Kafka consumers, the consumers will commit their offsets to a special topic: \__consumer_offsets. If you mirror this topic to your DR cluster, when consumers start consuming from the DR cluster they will be able to pick up their old offsets and continue from where they left off.</p>
</div>
<div class="paragraph">
<p>First, there is no guarantee that offsets in the primary cluster will match those in the secondary cluster.</p>
</div>
<div class="paragraph">
<p>Second, even if you started mirroring immediately when the topic was first created and both the primary and the DR topics start with 0, producer retries can cause offsets to diverge. Simply put, there is no existing Kafka mirroring solution that preserves offsets between primary and DR clusters.</p>
</div>
<div class="paragraph">
<p>Third, even if the offsets were perfectly preserved, because of the lag between primary and DR clusters and because Kafka currently lacks transactions, an offset committed by a Kafka consumer may arrive ahead or behind the record with this offset.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_19.png" alt="kafka 19">
</div>
</div>
<div class="paragraph">
<p>In these cases, you need to accept some duplicates if the latest committed offset in the DR site is older than the one committed on the primary or if the offsets in the records in the DR site are ahead of the primary due to retries.</p>
</div>
</div>
<div class="sect3">
<h4 id="_time_based_failover">Time-based failover</h4>
<div class="paragraph">
<p>If you are using really new (0.10.0 and above) Kafka consumers, each message includes a timestamp indicating the time the message was sent to Kafka. In really new Kafka versions (0.10.1.0 and above), the brokers include an index and an API for looking up offsets by the timestamp. So, if you failover to the DR cluster and you know that your trouble started at 4:05 A.M., you can tell consumers to start processing data from 4:03 A.M. There will be some duplicates from those two minutes, but it is probably better than other alternatives and the behavior is much easier to explain to everyone in the company.</p>
</div>
</div>
<div class="sect3">
<h4 id="_external_offset_mapping">External offset mapping</h4>
<div class="paragraph">
<p>When discussing mirroring the offsets topic, one of the biggest challenges with that approach is the fact that offsets in primary and DR clusters can diverge. With this in mind, some organizations choose to use an external data store, such as Apache Cassandra, to store mapping of offsets from one cluster to another.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_after_the_failover">AFTER THE FAILOVER</h3>
<div class="paragraph">
<p>It is tempting to simply modify the mirroring processes to reverse their direction and simply start mirroring from the new primary to the old one. However, this leads to two important questions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>How do we know where to start mirroring? We need to solve the same problem we have for all our consumers for the mirroring application itself. And remember that all our solutions have cases where they either cause duplicates or miss data—often both.</p>
</li>
<li>
<p>In addition, for reasons we discussed above, it is likely that your original primary will have events that the DR cluster does not. If you just start mirroring new data back, the extra history will remain and the two clusters will be inconsistent.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For this reason, the simplest solution is to first scrape the original cluster—delete all the data and committed offsets and then start mirroring from the new primary back to what is now the new DR cluster. This gives you a clean slate that is identical to the new primary.</p>
</div>
</div>
<div class="sect2">
<h3 id="_apache_kafka_s_mirrormaker">Apache Kafka’s MirrorMaker</h3>
<div class="paragraph">
<p>MirrorMaker runs a thread for each consumer. Each consumer consumes events from the topics and partitions it was assigned on the source cluster and uses the shared producer to send those events to the target cluster. Every 60 seconds (by default), the consumers will tell the producer to send all the events it has to Kafka and wait until Kafka acknowledges these events. Then the consumers contact the source Kafka cluster to commit the offsets for those events.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="kafka_20.png" alt="kafka 20">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_deploying_mirrormaker_in_production">Deploying MirrorMaker in Production</h3>
<div class="paragraph">
<p>run MirrorMaker at the destination datacenter. If there is a network partition and you lose connectivity between the datacenters, having a consumer that is unable to connect to a cluster is much safer than a producer that can’t connect. If the events were already consumed and MirrorMaker can’t produce them due to network partition, there is always a risk that these events will accidentally get lost by MirrorMaker.</p>
</div>
<div class="paragraph">
<p>When do you have to consume locally and produce remotely? The answer is when you need to encrypt the data while it is transferred between the datacenters but you don’t need to encrypt the data inside the datacenter.</p>
</div>
<div class="paragraph">
<p>it is important to remember to monitor it as follows:
- Lag monitoring<br>
image::kafka_21.png[]
- Metrics monitoring
	-- Consumer: fetch-size-avg, fetch-size-max, fetch-rate, fetch-throttle-time-avg, and fetch-throttle-time-max
	-- Producer: batch-size-avg, batch-size-max, requests-in-flight, and record-retry-rate
	-- Both: io-ratio and io-wait-ratio</p>
</div>
</div>
<div class="sect2">
<h3 id="_tuning_mirrormaker">Tuning MirrorMaker</h3>
<div class="paragraph">
<p>Kafka ships with the kafka-performance-producer tool. Use it to generate load on a source cluster and then connect MirrorMaker and start mirroring this load. Test MirrorMaker with 1, 2, 4, 8, 16, 24, and 32 consumer threads. Watch where performance tapers off and set num.streams just below this point. If you are consuming or producing compressed events (recommended, since bandwidth is the main bottleneck for cross-datacenter mirroring), MirrorMaker will have to decompress and recompress the events. This uses a lot of CPU, so keep an eye on CPU utilization as you increase the number of threads. Using this process, you will find the maximum throughput you can get with a single MirrorMaker instance. If it is not enough, you will want to experiment with additional instances and after that, additional servers.</p>
</div>
<div class="paragraph">
<p>If you are running MirrorMaker across datacenters, you want to optimize the network configuration in Linux as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Increase TCP buffer size (net.core.rmem_default, net.core.rmem_max, net.core.wmem_default, net.core.wmem_max, net.core.optmem_max)</p>
</li>
<li>
<p>Enable automatic window scaling (sysctl –w net.ipv4.tcp_window_scaling=1 or add net.ipv4.tcp_window_scaling=1 to /etc/sysctl.conf)</p>
</li>
<li>
<p>Reduce the TCP slow start time (set /proc/sys/net/ipv4/tcp_slow_start_after_idle to 0)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Another method is to do several thread dumps (using jstack) and see if the MirrorMaker threads are spending most of the time in poll or in send—more time spent polling usually means the consumer is the bottleneck, while more time spent sending shift points to the producer.</p>
</div>
<div class="paragraph">
<p>If you need to tune the producer, the following configuration settings can be useful:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>max.in.flight.requests.per.connection</p>
</li>
<li>
<p>linger.ms and batch.size</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following consumer configurations can increase throughput for the consumer:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The partition assignment strategy in MirrorMaker (i.e., the algorithm that decides which consumer is assigned which partitions) defaults to range. There are benefits to range strategy, which is why it is the normal default for consumers, but it can lead to uneven assignment of partitions to consumers. For MirrorMaker, it is usually better to change the strategy to round robin, especially when mirroring a large number of topics and partitions. You set this by adding partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor to the consumer properties file.</p>
</li>
<li>
<p>fetch.max.bytes</p>
</li>
<li>
<p>fetch.min.bytes and fetch.max.wait</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_links">Links</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="queues.html">Parent</a></p>
</div>
</div>
</div>
                        </div>
                        <div class="height-40px"></div>
                        <div>
                            <div id="remark42" aria-live="polite">
                                <noscript>Please enable JavaScript to view the comments</noscript>
                            </div>
                        </div>

                    </article>
                </div>
            </main>
        </div>
    </div>
</div>

<footer id="site-footer">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <p class="copyright">&copy; 2020 Konoplev.me</p>
            </div>
        </div>
    </div>
</footer>

<!-- Mobile Menu -->
<div class="overlay overlay-hugeinc">
    <button type="button" class="overlay-close"><span class="ion-ios-close-empty"></span></button>
    <nav>
        <ul>
            <li><a href="index.html">Garden</a></li>
            <li><a href="http://konoplev.me">Blog</a></li>
        </ul>
    </nav>
</div>

<script src="js/script.js" type="module"></script>
<script>
    var remark_config = {
        host: "https://comments.konoplev.me",
        site_id: 'konoplev',
    };

    (function (c) {
        for (var i = 0; i < c.length; i++) {
            var d = document, s = d.createElement('script');
            s.src = remark_config.host + '/web/' + c[i] + '.js';
            s.defer = true;
            (d.head || d.body).appendChild(s);
        }
    })(remark_config.components || ['embed']);
</script>

</body>
</html>
